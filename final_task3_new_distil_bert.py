# -*- coding: utf-8 -*-
"""final_task3_new.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QXsYvFNupWl6v9NyrGGYCjhPCP2tbKpZ
"""

args = {}
args["input_model_path"] = "distilbert/distilbert-base-uncased"

import torch
from tqdm import tqdm
import time
from torch.utils.data import Dataset
from torchvision import datasets
from torchvision.transforms import ToTensor
from sklearn.metrics import accuracy_score
import os
import gc
import pandas as pd
import torch.nn.functional as F
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoConfig, AutoModelForTokenClassification, AutoModel, BertPreTrainedModel
from torch import cuda
import numpy as np

MAX_LEN = 512

def tokenize_sent(sentence, tokenizer):

    tokenized_sentence = []
    sentence = str(sentence).strip()

    for word in sentence.split():
        tokenized_word = tokenizer.tokenize(word)
        n_subwords = len(tokenized_word)
        tokenized_sentence.extend(tokenized_word)

    return tokenized_sentence

import torch
from tqdm import tqdm
import time
from torch.utils.data import Dataset
from torchvision import datasets
from torchvision.transforms import ToTensor
from sklearn.metrics import accuracy_score
import os
import gc
import argparse
import pandas as pd
import torch.nn.functional as F
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoConfig, AutoModelForTokenClassification, AutoModel, BertPreTrainedModel, DistilBertPreTrainedModel, DistilBertModel, DistilBertTokenizer
from torch import cuda
from torch.utils.data import ConcatDataset
from data_loader import load_qqp,load_paws
import pickle

input_path = './'
num_labels = 2
BATCH_SIZE = 32

class LossFunction(nn.Module):
    def forward(self, probability):
        loss = torch.log(probability)
        loss = -1 * loss
        # print(loss)
        loss = loss.mean()
        # print(loss)
        return loss



class MainModel(DistilBertPreTrainedModel):
    def __init__(self, config, loss_fn = None):
        super(MainModel,self).__init__(config)
        self.num_labels = num_labels
        self.loss_fn = loss_fn
        config.output_hidden_states = True
        self.distilbert = DistilBertModel.from_pretrained(args["input_model_path"],config = config)
        self.classifier = nn.Linear(768, self.num_labels)

    def forward(self, input_ids, attention_mask, labels,device):

        output = self.distilbert(input_ids, attention_mask = attention_mask)
        output = output.last_hidden_state
        output = output[:,0,:]
        classifier_out = self.classifier(output)
        main_prob = F.softmax(classifier_out, dim = 1)
        main_gold_prob = torch.gather(main_prob, 1, labels)
        loss_main = self.loss_fn.forward(main_gold_prob)
        return loss_main,main_prob

tokenizer = DistilBertTokenizer.from_pretrained(args["input_model_path"])
model = MainModel.from_pretrained(args["input_model_path"], loss_fn = LossFunction())
print(model)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)
model.to(device)

df = pd.read_csv("questions.csv")

#df.isna().sum()

#df = df.dropna(subset = ["question1","question2"])

#df.isna().sum()

texts = []

for i in range(len(df["question1"])):
    if(type(df["question1"][0]) != type(df["question1"][i])): print(df["question1"][i], df["question2"][i], i, type(df["question1"][i]),type(df["question2"][i]))
    else: texts.append(str(df["question1"][i]) + "  " + str(df["question2"][i]))

#print(texts[0])

lowercase_texts = [string.lower() for string in texts]

import shap

masker = shap.maskers.Text(tokenizer=r"\w+|[^\w\s]")

#create prediction function

max_len = 512

def predict_f(texts):
    f_inputs = {}
    f_inputs["target"] = []
    #f_inputs['token_type_ids'] = []
    f_inputs['mask'] = []
    f_inputs['ids'] = []
    for text in texts:
        if "  " in text:
            sent1 = text.split("  ")[0]
            sent2 = text.split("  ")[1]
        else:
            sent1 = text
            sent2 = ""

        if len(sent1)>0 and sent1[len(sent1)-1] == " ":
            sent1 = sent1[:-1]
        if len(sent2)>0 and sent2[0] == " ":
            sent2 = sent2[1:]

        label = 0
        target = []
        target.append(label)

        token_type_ids = []
        token_type_ids.append(0)
        sent1 = tokenize_sent(sent1,tokenizer)
        sent2 = tokenize_sent(sent2,tokenizer)
        for i in enumerate(sent1):
            token_type_ids.append(0)
        token_type_ids.append(1)
        for i in enumerate(sent2):
            token_type_ids.append(1)
        token_type_ids.append(1)
        input_sent = ['[CLS]'] + sent1 + ['[SEP]'] + sent2 + ['[SEP]']
        input_sent = input_sent + ['[PAD]' for _ in range(max_len - len(input_sent))]
        token_type_ids = token_type_ids + [0 for _ in range(max_len - len(token_type_ids))]
        attn_mask = [1 if tok != '[PAD]' else 0 for tok in input_sent]
        ids = tokenizer.convert_tokens_to_ids(input_sent)

        f_inputs["target"].append(target)
        #f_inputs['token_type_ids'].append(token_type_ids)
        f_inputs['mask'].append(attn_mask)
        f_inputs['ids'].append(ids)

    f_inputs["target"] = torch.tensor(f_inputs["target"], dtype=torch.long)
    #f_inputs['token_type_ids'] = torch.tensor(f_inputs['token_type_ids'], dtype=torch.long)
    f_inputs['mask'] = torch.tensor(f_inputs['mask'], dtype=torch.long)
    f_inputs['ids'] = torch.tensor(f_inputs['ids'], dtype=torch.long)

    input_ids = f_inputs['ids'].to(device, dtype=torch.long)
    mask = f_inputs['mask'].to(device, dtype=torch.long)
    targets = f_inputs['target'].to(device, dtype=torch.long)
    #token_type_ids = f_inputs['token_type_ids'].to(device, dtype = torch.long)

    with torch.no_grad():
        loss_main,main_prob = model(input_ids=input_ids, attention_mask=mask, labels=targets, device = device)
    return main_prob

explainer = shap.Explainer(predict_f, masker)

predictions = []
sent1 = []
sent2 = []
true_out = []
nd_words = []
d_words = []
output_prob = []

def give_class (output_tensor):
    class_labels = ["not duplicate","duplicate"]
    predicted_class_idx = torch.argmax(output_tensor).item()
    predicted_class = class_labels[predicted_class_idx]
    return predicted_class

def sort_list (x):
    sorted_list = sorted(x, key=lambda x: x[1], reverse=True)
    return sorted_list

idx = 0
lowercase_texts = lowercase_texts[:15000]
print(len(lowercase_texts))

for item in lowercase_texts:
    texts = [item]
    predicted_out = predict_f(texts)
    out_pred_arr = []
    out_pred_arr.append(predicted_out[0][0].item())
    out_pred_arr.append(predicted_out[0][1].item())
    output_prob.append(out_pred_arr)
    predictions.append(give_class(predicted_out[0]))
    sent1.append(df["question1"][idx])
    sent2.append(df["question2"][idx])
    true_out.append(df["is_duplicate"][idx])
    shap_values = explainer(texts)

    d_arr = []
    nd_arr = []

    for item in shap_values[0]:
        d_arr.append((item[0].data.strip(),round(item[0].values.item(),4),round(item[0].base_values.item(),4)))
        nd_arr.append((item[1].data.strip(),round(item[1].values.item(),4),round(item[1].base_values.item(),4)))

    d_words.append(sort_list(d_arr))
    nd_words.append(sort_list(nd_arr))
    idx+=1
    print(idx)



final_result = pd.DataFrame({
    "sent1":sent1,
    "sent2":sent2,
    "true_out":true_out,
    "d_words":d_words,
    "nd_words":nd_words,
    "predicted_out":predictions,
    "predicted_prob":output_prob
})

final_result.to_csv("/TIDF_80/tidf80-user/rohit/QQP/shap/QQP_pred_distil_bert_pretrained.csv")

print(shap_values.shape)
